{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af831406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96999bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "global_list = []\n",
    "\n",
    "def log2(x):\n",
    "  global_list.append(x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36645eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed x: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_with_print(x):\n",
    "  print(\"printed x:\", x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2_with_print)(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced498b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145e4614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "learning/45eac/tfrc/runtime/env_var_utils.cc:50\n",
      "\n",
      "Failed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "learning/45eac/tfrc/runtime/env_var_utils.cc:50\n",
      "\n",
      "Failed to get TPU metadata (tpu-env) from instance metadata for variable ALT: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "learning/45eac/tfrc/runtime/env_var_utils.cc:50\n",
      "\n",
      "Failed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "learning/45eac/tfrc/runtime/env_var_utils.cc:50\n",
      "\n",
      "Failed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "\n",
      "Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.\n",
      "Failed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "\n",
      "Failed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: DEADLINE_EXCEEDED: Timeout was reached\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:84\n",
      "\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1759127127.162108 1380225 common_lib.cc:532] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/libtpu_init_utils.cc:175\n",
      "/home/mihir.more/miniconda3/envs/myenv/lib/python3.11/site-packages/jax/_src/xla_bridge.py:131: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  50 | loss 0.0682 | acc 99.2%\n",
      "step 100 | loss 0.0535 | acc 99.2%\n",
      "step 150 | loss 0.0484 | acc 99.2%\n",
      "step 200 | loss 0.0459 | acc 99.2%\n",
      "step 250 | loss 0.0445 | acc 99.2%\n",
      "step 300 | loss 0.0437 | acc 99.2%\n",
      "step 350 | loss 0.0431 | acc 99.2%\n",
      "step 400 | loss 0.0427 | acc 99.4%\n",
      "\n",
      "Final params:\n",
      "w: [3.182745 3.107227]  b: 0.5888577103614807\n",
      "Final train accuracy: 0.9940000176429749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from jax import random, grad, jit\n",
    "\n",
    "# Generate toy 2D data (two Gaussian blobs) -----\n",
    "key = random.PRNGKey(0)\n",
    "n_per_class = 250\n",
    "\n",
    "key1, key2, key3 = random.split(key, 3)\n",
    "mean0 = jnp.array([-1.0, -1.0])\n",
    "mean1 = jnp.array([+1.0, +1.0])\n",
    "\n",
    "X0 = mean0 + 0.6 * random.normal(key1, (n_per_class, 2))\n",
    "X1 = mean1 + 0.6 * random.normal(key2, (n_per_class, 2))\n",
    "X = jnp.concatenate([X0, X1], axis=0)                             # (N, 2)\n",
    "y = jnp.concatenate([jnp.zeros(n_per_class), jnp.ones(n_per_class)])  # labels in {0,1}\n",
    "\n",
    "# Shuffle\n",
    "perm = random.permutation(key3, X.shape[0])\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# -- Model & loss -----\n",
    "def logits(params, X):\n",
    "    w, b = params\n",
    "    return X @ w + b  # (N,)\n",
    "\n",
    "# Binary cross-entropy via softplus for stability:\n",
    "# BCE = mean( softplus(logit) - y*logit )\n",
    "def loss_fn(params, X, y, l2=0.0):\n",
    "    z = logits(params, X)\n",
    "    data_loss = jnp.mean(jnp.logaddexp(0.0, z) - y * z)\n",
    "    reg = 0.5 * l2 * (jnp.sum(params[0] ** 2) + params[1] ** 2)\n",
    "    return data_loss + reg\n",
    "\n",
    "# --- Gradients (jax.grad) -----\n",
    "loss_grad = grad(loss_fn)\n",
    "\n",
    "# ---- Gradient descent loop -----\n",
    "def accuracy(params, X, y):\n",
    "    z = logits(params, X)\n",
    "    yhat = (z > 0).astype(y.dtype)\n",
    "    return jnp.mean(yhat == y)\n",
    "\n",
    "@jit\n",
    "def step(params, X, y, lr, l2):\n",
    "    g_w, g_b = loss_grad(params, X, y, l2)\n",
    "    w, b = params\n",
    "    w = w - lr * g_w\n",
    "    b = b - lr * g_b\n",
    "    return (w, b)\n",
    "\n",
    "# Init\n",
    "params = (jnp.zeros(2), 0.0)   # (w, b)\n",
    "lr = 0.5\n",
    "l2 = 1e-3\n",
    "num_steps = 400\n",
    "\n",
    "for t in range(1, num_steps + 1):\n",
    "    params = step(params, X, y, lr, l2)\n",
    "    if t % 50 == 0:\n",
    "        current_loss = loss_fn(params, X, y, l2)\n",
    "        acc = accuracy(params, X, y)\n",
    "        print(f\"step {t:3d} | loss {current_loss:.4f} | acc {acc*100:.1f}%\")\n",
    "\n",
    "w, b = params\n",
    "print(\"\\nFinal params:\")\n",
    "print(\"w:\", w, \" b:\", float(b))\n",
    "print(\"Final train accuracy:\", float(accuracy(params, X, y)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
